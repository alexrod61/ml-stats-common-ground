[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning and Statistics: A Common Ground",
    "section": "",
    "text": "Preface\nThroughout my journey as a postdoctoral fellow in the Master of Data Science (MDS) at the University of British Columbia, I became aware of the fascinating overlap between Machine Learning and Statistics. Many Data Science students usually come across common Machine Learning/Statistics concepts or ideas that might only differ in names. For instance, simple terms such as weights in supervised learning (and their equivalent statistical counterpart as regression coefficients) might be misleading for students starting their Data Science formation. On the other hand, from an instructor’s perspective in a Data Science program that subsets its courses in Machine Learning in Python and Statistics in R, regression courses in R also demand the inclusion of Python-related packages as alternative tools. In my MDS teaching experience, this is especially critical for students whose career plans lean towards industry where Python is more heavily used.\nAs a Data Science educator, I view this field as a substantial synergy between Machine Learning and Statistics. Nevertheless, I believe there are still many gaps to be addressed between both disciplines. Thus, closing these critical gaps is imperative in a domain with accelerated growth, such as Data Science. The MDS Stat-ML dictionary inspired me to write this book. It basically consists of common ground between foundational supervised learning models from Machine Learning and regression models commonly used in Statistics. I strive to explore common modelling approaches as a primary step while highlighting different terminology found in both fields. Furthermore, this discussion is not limited to a simple conceptual exploration. Hence, the second step is hands-on practice via the corresponding Python packages for Machine Learning and R for Statistics."
  },
  {
    "objectID": "audience-scope.html",
    "href": "audience-scope.html",
    "title": "Audience and Scope",
    "section": "",
    "text": "This book mainly focuses on regression analysis and its supervised learning counterpart. Thus, it is not introductory Statistics and Machine Learning material. Instead, the following topics are suggested as prerequisites:\n\nMutivariable Differential Calculus and Linear Algebra. Certain sections of each chapter pertain to modelling estimation. Therefore, topics such as partial derivatives and matrix algebra are a great asset. You can find helpful learning resources on the MDS webpage.\nBasic R programming. Knowledge of data wrangling and plotting through R {tidyverse} is recommended for hands-on practice via the examples provided in each one of the chapters of this book. The MDS courses DSCI 523 (Programming for Data Manipulation) and DSCI 531 (Data Visualization I) are ideal examples of this prerequisite.\nBasic Python programming. When necessary, Python {pandas} library will be used to perform data wrangling. The MDS course DSCI 511 (Programming for Data Science) is an ideal example of this prerequisite.\nFoundations of probability and basic distributional knowledge. The reader should be familiar with elemental discrete and continuous distributions since they are a vital component of any given regression or supervised learning model. The MDS course DSCI 551 (Descriptive Statistics and Probability for Data Science) is an ideal example of this prerequisite.\nFoundations of frequentist statistical inference. One of the Data Science paradigms to be covered in this book is statistical inference, i.e., identifying relationships between different variables in a given population or system of interest via a sampled dataset. I only aim to cover a frequentist approach using inferential tools such as parameter estimation, hypothesis testing, and confidence intervals. The MDS course DSCI 552 (Statistical Inference and Computation I) is an ideal example of this prerequisite.\nFoundations of supervised learning. The second Data Science paradigm to be covered pertains to prediction, which is core in Machine Learning. The reader should be familiar with basic terminology, such as training and testing data, overfitting, underfitting, cross-validation, etc. The MDS course DSCI 571 (Machine Learning I) provides these foundations.\nFoundations of feature and model selection. This prerequisite also relates to Machine Learning and its corresponding prediction paradigm. Basic knowledge of prediction accuracy and variable selection tools is recommended. The MDS course DSCI 573 (Feature and Model Selection) is an ideal example of this prerequisite."
  },
  {
    "objectID": "book-structure.html#data-science-workflow",
    "href": "book-structure.html#data-science-workflow",
    "title": "How this Book is Structured",
    "section": "Data Science Workflow",
    "text": "Data Science Workflow\nA crucial aspect of the practice of supervised learning is the need for a systematic Data Science workflow that will allow us to solve our respective inquiries in a transparent and reproducible way. Hence, Chapter 1 begins with the general definition of supervised learning and its two critical paradigms: inference and prediction. Moreover, its workflow is fully explained across seven sequential phases:\n\nDesign.\nData collection.\nExploratory data analysis.\nModelling.\nEstimation.\nResults.\nStorytelling.\n\nNow, suppose we do not follow a predefined workflow in practice. In that case, we might be at risk of incorrectly addressing these inquiries, translating into meaningless results outside the context of the problem we aim to solve. This is why the formation of a Data Scientist must stress this workflow from the very introductory stages.\nOn the other hand, even though this book has prerequisites related to the basics of probability via different distributions and the fundamentals of frequentist statistical inference, Chapter 1 also provides a handy summary of all these topics. We will check essential concepts such as random variables, sampling, parameter estimation via maximum likelihood, the process of hypothesis testing, delivery of confidence intervals, and how to obtain outcome predictions. Note these concepts are heavily related to the Data Science workflow depicted in Section 1.5 across its different phases."
  },
  {
    "objectID": "book-structure.html#mindmap-of-regression-analysis",
    "href": "book-structure.html#mindmap-of-regression-analysis",
    "title": "How this Book is Structured",
    "section": "Mindmap of Regression Analysis",
    "text": "Mindmap of Regression Analysis\nHaving defined the necessary statistical aspects to execute a proper supervised learning analysis, either inferential or predictive across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in Figure 1. Note these regression models can be split into two sets depending on whether the outcome of interest is continuous or discrete. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.\n\n\n\n\n\n\n\nFigure 1: Regression analysis mindmap depicting all modelling techniques to be explored in this book. These techniques are split into two big sets: continuous and discrete outcomes.\n\n\nAs we can see in the clouds of Figure 1, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in Chapter 2. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any Data Scientist must be familiar with in everyday practice."
  },
  {
    "objectID": "book-structure.html#dictionary-of-terminology",
    "href": "book-structure.html#dictionary-of-terminology",
    "title": "How this Book is Structured",
    "section": "Dictionary of Terminology",
    "text": "Dictionary of Terminology\nFinally, following the structure of the MDS Stat-ML dictionary, Appendix A contains a dictionary that follows the same format which contrasts the terminology a Data Scientist can find in Statistics and Machine Learning when performing supervised learning in practice. Note that terms will be coloured depending on their statistical or Machine Learning nature.\n\n\n\n\n\n\nHeads-up!\n\n\n\nReaders might be surprised that certain concepts refer to the same idea but with different terminology. Contrarily, in some other cases, the same terminology might refer to different concepts!"
  },
  {
    "objectID": "intro.html#sec-basics-prob",
    "href": "intro.html#sec-basics-prob",
    "title": "1  Introduction",
    "section": "1.1 Basics of Probability",
    "text": "1.1 Basics of Probability"
  },
  {
    "objectID": "intro.html#sec-basics-inf",
    "href": "intro.html#sec-basics-inf",
    "title": "1  Introduction",
    "section": "1.2 Basics of Frequentist Statistical Inference",
    "text": "1.2 Basics of Frequentist Statistical Inference"
  },
  {
    "objectID": "intro.html#sec-mle",
    "href": "intro.html#sec-mle",
    "title": "1  Introduction",
    "section": "1.3 What is Maximum Likelihood Estimation?",
    "text": "1.3 What is Maximum Likelihood Estimation?"
  },
  {
    "objectID": "intro.html#sec-sup-learning",
    "href": "intro.html#sec-sup-learning",
    "title": "1  Introduction",
    "section": "1.4 What is Supervised Learning?",
    "text": "1.4 What is Supervised Learning?"
  },
  {
    "objectID": "intro.html#sec-ds-workflow",
    "href": "intro.html#sec-ds-workflow",
    "title": "1  Introduction",
    "section": "1.5 The Data Science Workflow in Supervised Learning",
    "text": "1.5 The Data Science Workflow in Supervised Learning\nIt is time to review the so-called Data Science workflow\nFor further reference, you can see Figure 1.1\n\n\n\n\n\n\n\nFigure 1.1: Data Science workflow for inferential and predictive inquiries in regression analysis and supervised learning, respectively.\n\n\n\n1.5.1 Design\n\n\n1.5.2 Data Collection\n\n\n1.5.3 Exploratory Data Analysis\n\n\n1.5.4 Modelling\n\n\n1.5.5 Estimation\n\n\n1.5.6 Results\n\n\n1.5.7 Storytelling"
  },
  {
    "objectID": "ols-regression.html",
    "href": "ols-regression.html",
    "title": "2  Ordinary Least-Squares Regression",
    "section": "",
    "text": "This is a OLS.\n\n1 + 1\n\n[1] 2\n\n\n\\[\n\\begin{equation}\na = 3\n\\end{equation}\n\\tag{2.1}\\]\n\nlemurs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-08-24/lemur_data.csv')\n\n\nlibrary(dplyr)\nlibrary(knitr)\nlemur_data &lt;- lemurs %&gt;% \n  filter(taxon == \"ECOL\",\n         sex == \"M\",\n         age_category == \"adult\") %&gt;% \n  select(c(age_at_wt_mo, weight_g)) %&gt;% \n  rename(Age = age_at_wt_mo, \n         Weight = weight_g)\nkable(head(lemur_data))\n\n\n\nAge\nWeight\n\n\n\n129.90\n2805\n\n\n132.10\n3001\n\n\n140.32\n2429\n\n\n157.94\n2597\n\n\n164.58\n2497\n\n\n184.18\n2225\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |&gt;\n  mutate(\n    bill_ratio = bill_depth_mm / bill_length_mm,\n    bill_area  = bill_depth_mm * bill_length_mm\n  )\n\n1\n\nTake penguins, and then,\n\n2\n\nadd new columns for the bill ratio and bill area.\n\n\n\n\nR Code\nR Output\n\n\n\n\nhead(mtcars)\n\n\n\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
  },
  {
    "objectID": "binary-logistic-regression.html",
    "href": "binary-logistic-regression.html",
    "title": "3  Binary Logistic Regression",
    "section": "",
    "text": "We will proceed with the first generalized linear model (GLM), binary logistic regression.\n\n1 + 1\n\n[1] 2"
  }
]